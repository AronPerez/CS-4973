{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS4973/5483_assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wUL_Fy5qUDI"
      },
      "source": [
        "# UTSA CS 4973/5483: Assignment-3\n",
        "\n",
        "Spring 2021\n",
        "\n",
        "\n",
        "**Last Name - First Name - (abc123)**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM8b9KVYsETT"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "*   Epipolar Geometry\n",
        "*   Triangulation\n",
        "*   Sparse 3D Reconstruction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeYRnesWqvLm"
      },
      "source": [
        "# Overview\n",
        "\n",
        "Consider that you are given two stereo images. The end goal is to perform **Sparse 3D Reconstruction using Epipolar Geometry**. Following is the overview of the sparse 3D reconstruction process:\n",
        "\n",
        "\n",
        "1.   Given two stereo images, find sparse feature point correspondences ( > 8 ).\n",
        "2.   Use these correspondences to estimate the Fundamental Matrix (F) using the 8-Point Algorithm.\n",
        "3.   Intrinsic matrices establish the relationship between the image and camera coordinates. These matrices are fixed for a given camera. Calibration is performed once using traditional checkerboard pattern techniques to obtain Intrinsic Matrix (K).\n",
        "4.   Calculate the Essential Matrix (E) using the the fundamental matrix (F) and the camera intrinsic matrices (K1 & K2).\n",
        "5.   To align the two camera views, the extrinsic matrix of the first camera (extr1) is considered to be Identity. The extrinsic matrix of the second camera (extr2) establishes the transformation between the two camera cameras.\n",
        "6.   extr1 -> rotation (R1) = Identity, translation (T1) = zero. Use extr1 and K1 to obtain the projection matrix of the first camera (P1).\n",
        "7.   extr2 -> Use the Essential matrix (E) to predict the extrinsic matrix of the second camera (extr2). There will be 4 estimates of which the one for which most of the projected 3D points are in front of both cameras. Use this extrinsic matrix to obtain the projection matrix of the second camera (P2).\n",
        "8.   The camera projection matrices (P1 & P2) can be decomposed into the rotation matrices (R1 & R2) and the camera translation vectors (T1 & T2).\n",
        "9.   Use the projection matrices P1 & P2 and given points to perform triangulation and obtain the sparse 3D reconstruction of the image.\n",
        "10.   Validate the sparse 3D reconstruction by computing the reprojection error.\n",
        "\n",
        "\n",
        "In order to obtain a depth value for the entire image (almost all pixels) we need to perform **Dense 3D Reconstruction using Stereo Rectification -- out of scope of this assignment**. Following is the overview of the dense 3D reconstruction process:\n",
        "\n",
        "1.   The steps 1-8 mentioned above are followed to obtain these values -- Fundamental matric (F), Essential matrix (E), projection matrices (P1 & P2), rotation matrices (R1 & R2), translation vectors (T1 & T2), intrinsic matrices (K1 & K2).\n",
        "2.   Perform Stereo Rectification to align both the images in a way that the epipolar lines in both the images are horizontal lines.\n",
        "3.   Once the images are rectified, we select every pixel in the image-1 and perform a linear search for the corresponding pixel in image-2 on this horizontal epipolar line.\n",
        "4.   We obtain correspondences for each and every pixel which can be used to compute the disparity map between the two images.\n",
        "5.   Use the disparity map to compute the depth map, which is the dense 3D reconstruction of the two stereo images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut2_4G75zYrm"
      },
      "source": [
        "# Sparse 3D Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9apbZGptej6"
      },
      "source": [
        "# Add your imports here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "877KvWMUM9Sl"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPAqyCWmM_17"
      },
      "source": [
        "# Base path where you should have all input data/images. Use this while reading.\n",
        "# This is the path that will be used by the TA - if you don't have this properly, the TA will not accept the submission.\n",
        "basePath = \"/content/drive/My Drive/Colab Notebooks/Computer Vision/images/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwSS1yl5zgh6"
      },
      "source": [
        "## Sparse Feature Point Correspondence\n",
        "(20 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikVaeecIAdpd"
      },
      "source": [
        "# Read the two images I1 & I2 and convert them to grayscale\n",
        "# Plot the two images next to each other as subplots with the size of the entire figure as 20x10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nThjBS593ql0"
      },
      "source": [
        "# Perform ORB feature detection\n",
        "# Plot the features on the two images next to each other as subplots with the size of the entire figure as 20x10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9hBtJQ_4Rm2"
      },
      "source": [
        "# Perform feature matching and extract only the top 72 matches\n",
        "# Obtain the correspondences in terms of pixel locations in the left and right image. Name these correspondences as 'pts1' & 'pts2'\n",
        "# Plot the matches on the two images next to each other as subplots with the size of the entire figure as 20x10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfgkXsWP4oQC"
      },
      "source": [
        "# Load manually cleaned point correspondences for the temple stereo images\n",
        "# CODE PROVIDED & COMMENTED\n",
        "# pts1 & pts2 arrays to be used by the TA during grading\n",
        "\n",
        "# test = np.load(basePath+\"temple_points.npz\")\n",
        "# pts1 = test['pts1']\n",
        "# pts2 = test['pts2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYJ1EPLq25Tr"
      },
      "source": [
        "## Fundamental Matrix (F) using the 8-Point Algorithm\n",
        "(40 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOcTwCizCyRu"
      },
      "source": [
        "# Write a function for the 8-Point Algorithm.\n",
        "# Input -- pts1, pts2, M (scale factor to normalize - max of the image width or height)\n",
        "# Output -- Fundamental Matrix F\n",
        "\n",
        "def eightpoint(pts1, pts2, M):\n",
        "    # REPLACE with your implementation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2qRa2-xDjR8"
      },
      "source": [
        "# Call the above created function for the 8-Point Algorithm\n",
        "# Store and print the returned Fundamental Matrix F\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtw8UzKs38mO"
      },
      "source": [
        "## Intrinsic Matrices K-1 & K-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1akejjKRAwoK"
      },
      "source": [
        "# Load the intrinsic matrices -- CODE PROVIDED\n",
        "intrinsics = np.load(basePath+\"intrinsics.npz\")\n",
        "K1, K2 = intrinsics['K1'], intrinsics['K2']\n",
        "print(K1)\n",
        "print(K2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73gR0Ib14haR"
      },
      "source": [
        "## Estimate Essential Matrix (E)\n",
        "(5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MlFMMUASBOc"
      },
      "source": [
        "# Write a function to estimate the essential matrix\n",
        "# Input -- F, K1, K2\n",
        "# Output -- E\n",
        "\n",
        "def essentialMatrix(F, K1, K2):\n",
        "    # REPLACE with your implementation\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LREOKUmiLEfh"
      },
      "source": [
        "# Call the above created function\n",
        "# Store and print the returned Essential Matrix E\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2T2ubXi5evd"
      },
      "source": [
        "## Camera-1 Projection\n",
        "(5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ahmnRYpTegz"
      },
      "source": [
        "# extr1 = [R | T] with R = I and T = 0-> 3x4\n",
        "# Compute and print the projection matrix of the first camera (P1) using K1 and ext1 -> 3x4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MwjvsZCFn8V"
      },
      "source": [
        "## Triangulation\n",
        "(40 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGcpQLUMWRWz"
      },
      "source": [
        "# Write a function to perform triangulation\n",
        "# Input: P1 - camera-1 projection matrix -> 3x4\n",
        "#        P2 - camera-2 projection matrix -> 3x4\n",
        "#        pts1 - points from image-1 -> Nx2\n",
        "#        pts2 - corresponding points from image-2 -> Nx2\n",
        "# Output: pts3D - triangulated 3D points -> Nx3\n",
        "\n",
        "def triangulate(P1, P2, pts1, pts2):\n",
        "    # REPLACE with your implementation\n",
        "\n",
        "    # Compute the 3D point for each point correspondence by doing the following:\n",
        "    #     Create the array A consisting of 2 rows each from corresponding points from pts1 and pts2\n",
        "    #     Perform SVD of 'A' to obtain the 3D point 'X'\n",
        "    #     Normailze the points to a scale and append to the list of triangulated 3D points\n",
        "    # Convert homogeneous 3D points to be heterogeneous 3D points\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_29dVsQdEkYT"
      },
      "source": [
        "## Camera-2 Projection\n",
        "(20 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xexrp9XUg9n"
      },
      "source": [
        "# Function to obtain the 4 possibilities for the camera-2 extrinsic matrix (extr2) -- CODE PROVIDED\n",
        "# Input -- Essential Matrix E\n",
        "# Ouput -- extr2\n",
        "\n",
        "def camera2(E):\n",
        "    U,S,V = np.linalg.svd(E)\n",
        "    m = S[:2].mean()\n",
        "    E = U.dot(np.array([[m,0,0], [0,m,0], [0,0,0]])).dot(V)\n",
        "    U,S,V = np.linalg.svd(E)\n",
        "    W = np.array([[0,-1,0], [1,0,0], [0,0,1]])\n",
        "\n",
        "    if np.linalg.det(U.dot(W).dot(V))<0:\n",
        "        W = -W\n",
        "\n",
        "    extr2s = np.zeros([3,4,4])\n",
        "    extr2s[:,:,0] = np.concatenate([U.dot(W).dot(V), U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1)\n",
        "    extr2s[:,:,1] = np.concatenate([U.dot(W).dot(V), -U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1)\n",
        "    extr2s[:,:,2] = np.concatenate([U.dot(W.T).dot(V), U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1)\n",
        "    extr2s[:,:,3] = np.concatenate([U.dot(W.T).dot(V), -U[:,2].reshape([-1, 1])/abs(U[:,2]).max()], axis=1)\n",
        "\n",
        "    return extr2s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yUphkPjUjFM"
      },
      "source": [
        "# Call the above function to obtain the 4 possibilities for extr2 -- CODE PROVIDED\n",
        "extr2s = camera2(E)\n",
        "print(extr2s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMzb5JHCVDy0"
      },
      "source": [
        "# Use the 'triangulate' function to figure out the correct value for extr2 from the 4 possibilities obtained above\n",
        "# For each possibility:\n",
        "#     Compute P2 using K2 and extr2\n",
        "#     Use this P2 to triangulate and obtain the 3D points\n",
        "# The extr2 for which all the triangulated 3D points are in front of both cameras (i.e., z > 0) is the correct extr2\n",
        "\n",
        "\n",
        "# REPLACE with your implementation\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcE3amjarTe_"
      },
      "source": [
        "# Print the final correct extr2 and the corresponding P2 -- CODE PROVIDED\n",
        "print(extr2)\n",
        "print(P2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jllmUT7YJ-bU"
      },
      "source": [
        "## Projection Matrix Decomposition\n",
        "(5 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyA5yjce_-uL"
      },
      "source": [
        "# Call the cv2.decomposeProjectionMatrix() function to decompose the projections matrices (P1 & P2) to obtain K, R, T separately for each of them\n",
        "# Print all 6 - K1 (3x3), K2 (3x3), R1 (3x3), R2 (3x3), T1 (3x1), T2 (3x1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te19O_zhKBbK"
      },
      "source": [
        "## 3D Reconstruction\n",
        "(10 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWwqodZAWFun"
      },
      "source": [
        "# Perform Sparse 3D Reconstruction\n",
        "# Call the 'triangulate' function using the final camera projection matrices (P1 & P2) and the points (pts1 & pts2) to obtain the final set of 3D points (pts3D)\n",
        "# Use plotly.express to plot an interactable 3D Scatter Plot for the 3D points (pts3D)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDAR9rSlLkXE"
      },
      "source": [
        "## Reprojection Error\n",
        "(20 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmKSyZJtq_XV"
      },
      "source": [
        "# Write a function to perform triangulation and obtain the reprojection error\n",
        "# Input: P1 - camera-1 projection matrix -> 3x4\n",
        "#        P2 - camera-2 projection matrix -> 3x4\n",
        "#        pts1 - points from image-1 -> Nx2\n",
        "#        pts2 - corresponding points from image-2 -> Nx2\n",
        "# Output: reprojection_error -> average Eucildean error of reprojection\n",
        "\n",
        "def reprojection_error(P1, P2, pts1, pts2):\n",
        "    # REPLACE with your implementation\n",
        "\n",
        "    # Compute the 3D point for each point correspondence by doing the following:\n",
        "    #     Create the array A consisting of 2 rows each from corresponding points from pts1 and pts2\n",
        "    #     Perform SVD of 'A' to obtain the 3D point 'X'\n",
        "    #     Normailze the points to a scale and append to the list of triangulated 3D points\n",
        "\n",
        "    # Reproject the 3D points using P1 to obtain pts1_reprojected\n",
        "    # Reproject the 3D points using P2 to obtain pts2_reprojected\n",
        "    # Normalize both and convert them to heterogeneous coordinates\n",
        "    # Compare (pts1 -- pts1_reprojected) and (pts1 -- pts1_reprojected)\n",
        "    # Use Eucildean distance between each point to compute and return the average reprojection error\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3z2Wa82-KVE"
      },
      "source": [
        "# Call the above functin to obtain and print the reprojection error for the ORB features points 'pts1' and 'pts2' -- CODE PROVIDED\n",
        "print(reprojection_error(P1, P2, pts1, pts2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJSFgNBQrhQU"
      },
      "source": [
        "#Submission Instructions\n",
        "\n",
        "\n",
        "\n",
        "1.   Complete all tasks above\n",
        "2.   Export this notebook as .ipynb\n",
        "      (File > Download as ipynb)\n",
        "3.   Upload the .ipynb file on Blackboard - **file MUST contain the output for ALL cells**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lGvLE9H6ptL"
      },
      "source": [
        "##Rubric\n",
        "\n",
        "*   (20 points) Sparse feature point correspondence\n",
        "*   (40 points) Fundamental Matrix (F) using the 8-Point Algorithm\n",
        "*   (05 points) Estimate Essential Matrix (E)\n",
        "*   (05 points) Camera-1 Projection\n",
        "*   (40 points) Triangulation\n",
        "*   (20 points) Camera-2 Projection\n",
        "*   (05 points) Projection Matrix Decomposition\n",
        "*   (10 points) 3D Reconstruction\n",
        "*   (20 points) Reprojection Error"
      ]
    }
  ]
}